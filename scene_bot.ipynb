{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scene-bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20ce482705bc4eeb97fe85acb01b1470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32047675a8ff4bfb87ab41fb128834d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_364d9604145149fe9f77587178fbc72f",
              "IPY_MODEL_fc37479025ff43028575de2d8021f70a"
            ]
          }
        },
        "32047675a8ff4bfb87ab41fb128834d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "364d9604145149fe9f77587178fbc72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4540115f53634dd58f54ac868326ded7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 642,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 642,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1960aa027004f07aeb3ee044232aab5"
          }
        },
        "fc37479025ff43028575de2d8021f70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35ddc1c6f3a8446e9bf3a89f5b981485",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 642/642 [00:00&lt;00:00, 888B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00a7f27146064952945d0101d8dd28f3"
          }
        },
        "4540115f53634dd58f54ac868326ded7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1960aa027004f07aeb3ee044232aab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35ddc1c6f3a8446e9bf3a89f5b981485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00a7f27146064952945d0101d8dd28f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "faea6a4f5643427d8c4af1df179d1964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_894c9e0d45c94e91935a972d2a37166e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8cf4ab1a7fda4516bcf061a72d6c5eaa",
              "IPY_MODEL_e60959f0a9ab4e179305a995159547f6"
            ]
          }
        },
        "894c9e0d45c94e91935a972d2a37166e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8cf4ab1a7fda4516bcf061a72d6c5eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d9522a0651e44a5b8113f3c0772e9ab8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1752292117,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1752292117,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96e2da5f119d4d47987c909e1b882bba"
          }
        },
        "e60959f0a9ab4e179305a995159547f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b88aa89b18bf451abe403c8b19835ec2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.75G/1.75G [00:51&lt;00:00, 34.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2b34bfedc2b44c8b8b6230cf77c2ebe"
          }
        },
        "d9522a0651e44a5b8113f3c0772e9ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96e2da5f119d4d47987c909e1b882bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b88aa89b18bf451abe403c8b19835ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2b34bfedc2b44c8b8b6230cf77c2ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATitA7T0Xa-J",
        "outputId": "55adbe68-7409-452d-fe9c-7df9e43336bf"
      },
      "source": [
        "#First please run this cell and fill in your google credentials (If worried I suppose you can use \n",
        "#a burner account but this opens a private client into your own gsutil \n",
        "#that will allow you to get access to the public directories \n",
        "#of my storage bucket in which I have stored very large model files) this is a great way to store large\n",
        "#files pretty much for free if anyone is interested\n",
        "!git clone https://github.com/aidan-collins/scene-bot\n",
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers==2.8.0\n",
        "!pip install tensorboardX\n",
        "!pip install seqeval\n",
        "!pip install gsutil\n",
        "!gsutil config"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'scene-bot'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 60 (delta 15), reused 51 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.6.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.6.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (708.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 708.0MB 23kB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.7.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (5.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.9MB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.6.0+cu101 torchvision-0.7.0+cu101\n",
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573kB 9.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 15.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 63.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 62.4MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/3e/3a4546165383a5fc9f6f7ba15a261c768aee10662bb06105100d859e8940/boto3-1.16.35-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 69.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.17.0)\n",
            "Collecting botocore<1.20.0,>=1.19.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/f8/d355891fc244cb31ad8a30ce452efbf2b31a48da0239f220a871c54fe829/botocore-1.19.35-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.1MB 62.3MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 10.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.35->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=dba1deded10e9624c5652822a49050727ff71ec8494030cbb7f5c973d9b4559d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.35 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, transformers\n",
            "Successfully installed boto3-1.16.35 botocore-1.19.35 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.5.2 transformers-2.8.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=1490a5ba296eea865e212abee1fd491666a57093126fa82dfe4bb19cd00fd8d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting gsutil\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/ec/d79f528581310ee5332626458e7a07c2d9c019448cc7979d8163860b4d34/gsutil-4.57.tar.gz (2.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 7.7MB/s \n",
            "\u001b[?25hCollecting argcomplete>=1.9.4\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/d0/ee7fc6ceac8957196def9bfa3b955d02163058defd3edd51ef7b1ff2769e/argcomplete-1.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.6/dist-packages (from gsutil) (1.7)\n",
            "Collecting fasteners>=0.14.1\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Collecting gcs-oauth2-boto-plugin>=2.7\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/ab/3cc16742de84b76aa328c4b9e09fbf88447027827c12fb3913c5907be23b/gcs-oauth2-boto-plugin-2.7.tar.gz\n",
            "Collecting google-apitools>=0.5.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174kB 42.8MB/s \n",
            "\u001b[?25hCollecting httplib2>=0.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/ad/d9d9331850ea5bd4f5cb8c650c0bfa119a4abd6b0ad7c45b6506bc979fc0/httplib2-0.18.1-py3-none-any.whl (95kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 12.4MB/s \n",
            "\u001b[?25hCollecting google-reauth>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/69/e1/67ffaa3a645b86318ce30717af7145070ebccec5eef5c623ae08b86129b8/google_reauth-0.1.1-py2.py3-none-any.whl\n",
            "Collecting mock==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 9.7MB/s \n",
            "\u001b[?25hCollecting monotonic>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL>=0.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/86/e21398551956735fef8f7883908771445878ccb16cd17c0896176419cd75/pyOpenSSL-20.0.0-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting retry_decorator>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/e6/bedc75b264cbcbf6e6d0e5071d96d739f540fc09be31744a7a8824c02a8e/retry_decorator-1.1.1.tar.gz\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from gsutil) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata<4,>=0.23; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from argcomplete>=1.9.4->gsutil) (3.1.1)\n",
            "Collecting boto>=2.29.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from gcs-oauth2-boto-plugin>=2.7->gsutil) (4.1.3)\n",
            "Collecting pyu2f\n",
            "  Downloading https://files.pythonhosted.org/packages/29/b5/c1209e6cb77647bc2c9a6a1a953355720f34f3b006b725e303c70f3c0786/pyu2f-0.1.5.tar.gz\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 64.5MB/s \n",
            "\u001b[?25hCollecting cryptography>=3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.6MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata<4,>=0.23; python_version == \"3.6\"->argcomplete>=1.9.4->gsutil) (3.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=2.2.0->gcs-oauth2-boto-plugin>=2.7->gsutil) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=2.2.0->gcs-oauth2-boto-plugin>=2.7->gsutil) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=2.2.0->gcs-oauth2-boto-plugin>=2.7->gsutil) (0.4.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=3.2->pyOpenSSL>=0.13->gsutil) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=3.2->pyOpenSSL>=0.13->gsutil) (2.20)\n",
            "Building wheels for collected packages: gsutil, gcs-oauth2-boto-plugin, google-apitools, retry-decorator, pyu2f\n",
            "  Building wheel for gsutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gsutil: filename=gsutil-4.57-cp36-none-any.whl size=3340433 sha256=f5ef34b98a352b9c03f6184e1dd2ed4c1d4b45cf489cf565497f147fe8fc7935\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/18/1c/9f12d053973060e235d1ede1bf4dd530d30b72c572dec0021a\n",
            "  Building wheel for gcs-oauth2-boto-plugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gcs-oauth2-boto-plugin: filename=gcs_oauth2_boto_plugin-2.7-cp36-none-any.whl size=23203 sha256=f7a7658b8f71c0a2d5044948612a3600a7f82463a6949924897e98e7e12bf7c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/6b/7c/bd86832ceb17e0ae3d362c44f461832452eeaacddfcf9128ee\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp36-none-any.whl size=131043 sha256=dbab5732051acacd355d3657528b0e77870aab471a14d537c3262a73c88bdb5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for retry-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retry-decorator: filename=retry_decorator-1.1.1-py2.py3-none-any.whl size=3639 sha256=009aee8baefbd06b40603af11bc1f29579a3b4c067cf537acd680fe77bed8a1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/70/30/4af820545aa19a0d96f969ef5ecebbb9743fd89cf00db43273\n",
            "  Building wheel for pyu2f (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyu2f: filename=pyu2f-0.1.5-cp36-none-any.whl size=39389 sha256=9b15f2dbe3e7840deaa9863e44fcede86365ac275721f01ea25a7b09057a35df\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/74/4d/2a07cf37327596c99f570ebe983a9843cda0278ca36a27ad9d\n",
            "Successfully built gsutil gcs-oauth2-boto-plugin google-apitools retry-decorator pyu2f\n",
            "Installing collected packages: argcomplete, monotonic, fasteners, boto, pyu2f, google-reauth, httplib2, cryptography, pyOpenSSL, retry-decorator, gcs-oauth2-boto-plugin, google-apitools, pbr, mock, gsutil\n",
            "  Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "Successfully installed argcomplete-1.12.2 boto-2.49.0 cryptography-3.3.1 fasteners-0.15 gcs-oauth2-boto-plugin-2.7 google-apitools-0.5.31 google-reauth-0.1.1 gsutil-4.57 httplib2-0.18.1 mock-2.0.0 monotonic-1.5 pbr-5.5.1 pyOpenSSL-20.0.0 pyu2f-0.1.5 retry-decorator-1.1.1\n",
            "This command will create a boto config file at /root/.boto containing\n",
            "your credentials, based on your responses to the following questions.\n",
            "Please navigate your browser to the following URL:\n",
            "https://accounts.google.com/o/oauth2/auth?client_id=909320924072.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&access_type=offline&response_type=code\n",
            "In your browser you should see a page that requests you to authorize access to Google Cloud Platform APIs and Services on your behalf. After you approve, an authorization code will be displayed.\n",
            "\n",
            "Enter the authorization code: 4/1AY0e-g6QSevFLfPHTv1GpzusoSeghaUOFDgpGc2emRgS1pytd0JnwdpUUss\n",
            "\n",
            "Please navigate your browser to https://cloud.google.com/console#/project,\n",
            "then find the project you will use, and copy the Project ID string from the\n",
            "second column. Older projects do not have Project ID strings. For such projects,\n",
            " click the project and then copy the Project Number listed under that project.\n",
            "\n",
            "What is your project-id? optimusencoding\n",
            "\n",
            "gsutil developers rely on user feedback to make improvements to the\n",
            "tool. Would you like to send anonymous usage statistics to help\n",
            "improve gsutil? [y/N] N\n",
            "\n",
            "Boto config file \"/root/.boto\" created. If you need to use a proxy to\n",
            "access the Internet please see the instructions in that file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98d_kd6sa4xI",
        "outputId": "4aec4dca-379e-4f25-b3e2-f25cd7a3d892"
      },
      "source": [
        "!gsutil -m cp -r gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim /content\n",
        "!gsutil -m cp -r gs://bucket-o-words/gedi-runs/run-1/topic_scripts /content\n",
        "!gsutil -m cp -r gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small /content\n",
        "!gsutil -m cp -r gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five /content\n",
        "!mv /content/topic_scripts /content/scene-bot/GeDi\n",
        "!mv /content/topic_scripts_genres_3_small /content/scene-bot/GeDi\n",
        "!mv /content/final-writing-model-sim /content/scene-bot/GeDi\n",
        "!mv /content/base_writer-dialogpt2-small-five /content/scene-bot/GeDi\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/config.json...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/merges.txt...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/optimizer.pt...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/pytorch_model.bin...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/scheduler.pt...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/special_tokens_map.json...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/special_tokens_map.json...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/trainer_state.json...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/vocab.json...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/config.json...\n",
            "Copying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/merges.txt...\n",
            "Copying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/training_args.bin...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \r/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \r/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/tokenizer_config.json...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/pytorch_model.bin...\n",
            "Copying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/training_args.bin...\n",
            "/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \r/ [0/18 files][    0.0 B/  5.3 GiB]   0% Done                                   \rCopying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/vocab.json...\n",
            "Copying gs://bucket-o-words/gpt2-medium-runs/final-writing-model-sim/writer-model/checkpoint-108500/tokenizer_config.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/config.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/eval_results.txt...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/merges.txt...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/pytorch_model.bin...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/special_tokens_map.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/tokenizer_config.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/vocab.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-1/topic_scripts/training_args.bin...\n",
            "| [8/8 files][524.0 MiB/524.0 MiB] 100% Done                                    \n",
            "Operation completed over 8 objects/524.0 MiB.                                    \n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/config.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/eval_results.txt...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/merges.txt...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/pytorch_model.bin...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/special_tokens_map.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/tokenizer_config.json...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/training_args.bin...\n",
            "Copying gs://bucket-o-words/gedi-runs/run-3/topic_scripts_genres_3_small/vocab.json...\n",
            "/ [8/8 files][524.0 MiB/524.0 MiB] 100% Done                                    \n",
            "Operation completed over 8 objects/524.0 MiB.                                    \n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/pytorch_model.bin...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/optimizer.pt...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/merges.txt...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/config.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/scheduler.pt...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/special_tokens_map.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/tokenizer_config.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/pytorch_model.bin...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/merges.txt...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/vocab.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/trainer_state.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/vocab.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/training_args.bin...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/tokenizer_config.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/special_tokens_map.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/config.json...\n",
            "Copying gs://bucket-o-words/final-movie-language-model/base_writer-dialogpt2-small-five/checkpoint-7000/training_args.bin...\n",
            "- [17/17 files][  1.9 GiB/  1.9 GiB] 100% Done 105.8 MiB/s ETA 00:00:00         \n",
            "Operation completed over 17 objects/1.9 GiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954,
          "referenced_widgets": [
            "20ce482705bc4eeb97fe85acb01b1470",
            "32047675a8ff4bfb87ab41fb128834d0",
            "364d9604145149fe9f77587178fbc72f",
            "fc37479025ff43028575de2d8021f70a",
            "4540115f53634dd58f54ac868326ded7",
            "d1960aa027004f07aeb3ee044232aab5",
            "35ddc1c6f3a8446e9bf3a89f5b981485",
            "00a7f27146064952945d0101d8dd28f3",
            "faea6a4f5643427d8c4af1df179d1964",
            "894c9e0d45c94e91935a972d2a37166e",
            "8cf4ab1a7fda4516bcf061a72d6c5eaa",
            "e60959f0a9ab4e179305a995159547f6",
            "d9522a0651e44a5b8113f3c0772e9ab8",
            "96e2da5f119d4d47987c909e1b882bba",
            "b88aa89b18bf451abe403c8b19835ec2",
            "d2b34bfedc2b44c8b8b6230cf77c2ebe"
          ]
        },
        "id": "0k0w3xVYUdpS",
        "outputId": "611c572e-ac5c-4b02-bb27-a9fd5250d38d"
      },
      "source": [
        "#Experiment 1 finetuned GeDi (from Dialogpt-small base model) on seq len ~200 batching of 8 -> superior performance to larger model, seq_len, batch\n",
        "%cd\n",
        "%cd /content/scene-bot/GeDi\n",
        "import torch\n",
        "import numpy as np \n",
        "\n",
        "from modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "from transformers import ( GPT2Config, GPT2Tokenizer )\n",
        "mode = \"topic\"\n",
        "code_desired = \"true\"\n",
        "code_undesired = \"false\"\n",
        "model_type = 'gpt2'\n",
        "gen_type = \"gedi\"\n",
        "gen_model_name_or_path = \"microsoft/DialoGPT-large\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[\"gpt2\"]\n",
        "tokenizer = tokenizer_class.from_pretrained('microsoft/DialoGPT-large', do_lower_case=False) #we'll grab a copy of the public tokenizer as it was ot altered during training\n",
        "\n",
        "model = model_class.from_pretrained(gen_model_name_or_path)\n",
        "model = model.to(device)\n",
        "model = model.float()\n",
        "\n",
        "gedi_model_name_or_path = '/content/scene-bot/GeDi/topic_scripts'\n",
        "gedi_model = model_class.from_pretrained(gedi_model_name_or_path)\n",
        "gedi_model.to(device)\n",
        "\n",
        "#setting arguments for generation\n",
        "#max generation length\n",
        "gen_length = 200\n",
        "#omega from paper, higher disc_weight means more aggressive topic steering\n",
        "disc_weight = 30\n",
        "#1 - rho from paper, should be between 0 and 1 higher filter_p means more aggressive topic steering\n",
        "filter_p = 0.8\n",
        "#tau from paper, preserves tokens that are classified as correct topic\n",
        "target_p = 0.8\n",
        "#hyperparameter that determines class prior, set to uniform by default\n",
        "class_bias = 0\n",
        "\n",
        "if gen_length>1024:\n",
        "  length = 1024\n",
        "else:\n",
        "  length = gen_length\n",
        "\n",
        "\n",
        "print('Please input a movie genre to select from (try sci-fi, horror, comedy and romance for example).')\n",
        "secondary_code = 'sci-fi'\n",
        "secondary_code = input('Movie genre: ')\n",
        "bpe_tokens = tokenizer.encode(secondary_code)\n",
        "\n",
        "\n",
        "\n",
        "multi_code = tokenizer.encode(secondary_code)\n",
        "attr_class = 1\n",
        "\n",
        "text = \"\"\n",
        "num_turns = 5 #generation lengths are unexpected so no garuntees youll get to the end of your conversation\n",
        "for i in range(num_turns):\n",
        "  prompt = input('Your turn: ')\n",
        "  prompt = text + \" \" + prompt + \" \"  + tokenizer.eos_token\n",
        "  text_ids = tokenizer.encode(prompt)\n",
        "  encoded_prompts=torch.LongTensor(text_ids).unsqueeze(0).to(device)\n",
        "\n",
        "  generated_sequence = model.generate(input_ids=encoded_prompts,\n",
        "                                          pad_lens=None,\n",
        "                                            max_length= length,\n",
        "                                            top_k=None,\n",
        "                                            top_p=None,\n",
        "                                            repetition_penalty= 1.2,\n",
        "                                            rep_penalty_scale= 10,\n",
        "                                            eos_token_ids = tokenizer.eos_token_id,\n",
        "                                            pad_token_id = 0,\n",
        "                                            do_sample= False,\n",
        "                                            penalize_cond= True,\n",
        "                                            gedi_model= gedi_model,\n",
        "                                            tokenizer= tokenizer,\n",
        "                                            disc_weight= disc_weight,\n",
        "                                            filter_p = filter_p,\n",
        "                                            target_p = target_p,\n",
        "                                            class_bias = class_bias,\n",
        "                                            attr_class = attr_class,\n",
        "                                            code_0 = code_undesired,\n",
        "                                            code_1 = code_desired,\n",
        "                                            multi_code=multi_code\n",
        "                                            )\n",
        "\n",
        "  text =  tokenizer.decode(generated_sequence.tolist()[0], clean_up_tokenization_spaces=True)\n",
        "  print('\\n')\n",
        "  print(text)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/scene-bot/GeDi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20ce482705bc4eeb97fe85acb01b1470",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faea6a4f5643427d8c4af1df179d1964",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1752292117.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "no logit scale initialized for gpt2\n",
            "Please input a movie genre to select from (try sci-fi, horror, comedy and romance for example).\n",
            "Movie genre: horror\n",
            "Your turn: hello script writer what are you doing?\n",
            "GeDi estimates the probability that it sample is desired class is: 0.5077186822891235\n",
            "\n",
            "\n",
            " hello script writer what are you doing?<|endoftext|>Writing scripts.<|endoftext|>\n",
            "Your turn: oh naturally... thats your job\n",
            "GeDi estimates the probability that it sample is desired class is: 0.505055844783783\n",
            "\n",
            "\n",
            " hello script writer what are you doing?<|endoftext|>Writing scripts.<|endoftext|> oh naturally... thats your job<|endoftext|>I write the scripts myself actually. I'm a script editor and wrote the first one in my class last semester. It was pretty fun writing it all out.<|endoftext|>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0c8aa48a510c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mnum_turns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m#generation lengths are unexpected so no garuntees youll get to the end of your conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_turns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your turn: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mtext_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXfvUDQx83O"
      },
      "source": [
        "#Experiment 2 finetuned GeDi (from Dialogpt-medium base model) on seq len ~700 batching of 2 -> inferior performance to smaller model, seq_len, batch\n",
        "%cd\n",
        "%cd /content/scene-bot/GeDi\n",
        "import torch\n",
        "import numpy as np \n",
        "\n",
        "from modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "from transformers import ( GPT2Config, GPT2Tokenizer )\n",
        "mode = \"topic\"\n",
        "code_desired = \"true\"\n",
        "code_undesired = \"false\"\n",
        "model_type = 'gpt2'\n",
        "gen_type = \"gedi\"\n",
        "gen_model_name_or_path = \"microsoft/DialoGPT-large\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[\"gpt2\"]\n",
        "tokenizer = tokenizer_class.from_pretrained('microsoft/DialoGPT-large', do_lower_case=False) #we'll grab a copy of the public tokenizer as it was ot altered during training\n",
        "\n",
        "model = model_class.from_pretrained(gen_model_name_or_path)\n",
        "model = model.to(device)\n",
        "model = model.float()\n",
        "\n",
        "gedi_model_name_or_path = 'topic_scripts_genres_3_small'\n",
        "gedi_model = model_class.from_pretrained(gedi_model_name_or_path)\n",
        "gedi_model.to(device)\n",
        "\n",
        "#setting arguments for generation\n",
        "#max generation length\n",
        "gen_length = 200\n",
        "#omega from paper, higher disc_weight means more aggressive topic steering\n",
        "disc_weight = 30\n",
        "#1 - rho from paper, should be between 0 and 1 higher filter_p means more aggressive topic steering\n",
        "filter_p = 0.8\n",
        "#tau from paper, preserves tokens that are classified as correct topic\n",
        "target_p = 0.8\n",
        "#hyperparameter that determines class prior, set to uniform by default\n",
        "class_bias = 0\n",
        "\n",
        "if gen_length>1024:\n",
        "  length = 1024\n",
        "else:\n",
        "  length = gen_length\n",
        "\n",
        "\n",
        "print('Please input a movie genre to select from (try sci-fi, horror, comedy and romance for example).')\n",
        "secondary_code = 'sci-fi'\n",
        "secondary_code = input('Movie genre: ')\n",
        "bpe_tokens = tokenizer.encode(secondary_code)\n",
        "\n",
        "\n",
        "\n",
        "multi_code = tokenizer.encode(secondary_code)\n",
        "attr_class = 1\n",
        "\n",
        "text = \"\"\n",
        "num_turns = 5 #generation lengths are unexpected so no garuntees youll get to the end of your conversation\n",
        "for i in range(num_turns):\n",
        "  prompt = input('Your turn: ')\n",
        "  prompt = text + \" \" + prompt + \" \"  + tokenizer.eos_token\n",
        "  text_ids = tokenizer.encode(prompt)\n",
        "  encoded_prompts=torch.LongTensor(text_ids).unsqueeze(0).to(device)\n",
        "\n",
        "  generated_sequence = model.generate(input_ids=encoded_prompts,\n",
        "                                          pad_lens=None,\n",
        "                                            max_length= length,\n",
        "                                            top_k=None,\n",
        "                                            top_p=None,\n",
        "                                            repetition_penalty= 1.2,\n",
        "                                            rep_penalty_scale= 10,\n",
        "                                            eos_token_ids = tokenizer.eos_token_id,\n",
        "                                            pad_token_id = 0,\n",
        "                                            do_sample= False,\n",
        "                                            penalize_cond= True,\n",
        "                                            gedi_model= gedi_model,\n",
        "                                            tokenizer= tokenizer,\n",
        "                                            disc_weight= disc_weight,\n",
        "                                            filter_p = filter_p,\n",
        "                                            target_p = target_p,\n",
        "                                            class_bias = class_bias,\n",
        "                                            attr_class = attr_class,\n",
        "                                            code_0 = code_undesired,\n",
        "                                            code_1 = code_desired,\n",
        "                                            multi_code=multi_code\n",
        "                                            )\n",
        "\n",
        "  text =  tokenizer.decode(generated_sequence.tolist()[0], clean_up_tokenization_spaces=True)\n",
        "  print('\\n')\n",
        "  print(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4RyHHhWiNW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85b0864a-9e61-4edb-d761-1860cf2db548"
      },
      "source": [
        "%cd\n",
        "%cd /content/scene-bot/GeDi\n",
        "import torch\n",
        "import numpy as np \n",
        "\n",
        "from modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "from transformers import ( GPT2Config, GPT2Tokenizer )\n",
        "mode = \"topic\"\n",
        "code_desired = \"true\"\n",
        "code_undesired = \"false\"\n",
        "model_type = 'gpt2'\n",
        "gen_type = \"gedi\"\n",
        "gen_model_name_or_path = \"/content/scene-bot/GeDi/final-writing-model-sim/writer-model\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[\"gpt2\"]\n",
        "tokenizer = tokenizer_class.from_pretrained('microsoft/DialoGPT-small', do_lower_case=False) #we'll grab a copy of the public tokenizer as it was ot altered during training\n",
        "\n",
        "model = model_class.from_pretrained(gen_model_name_or_path)\n",
        "model = model.to(device)\n",
        "model = model.float()\n",
        "\n",
        "gedi_model_name_or_path = '/content/scene-bot/GeDi/topic_scripts'\n",
        "gedi_model = model_class.from_pretrained(gedi_model_name_or_path)\n",
        "gedi_model.to(device)\n",
        "\n",
        "#setting arguments for generation\n",
        "#max generation length\n",
        "gen_length = 200\n",
        "#omega from paper, higher disc_weight means more aggressive topic steering\n",
        "disc_weight = 30\n",
        "#1 - rho from paper, should be between 0 and 1 higher filter_p means more aggressive topic steering\n",
        "filter_p = 0.8\n",
        "#tau from paper, preserves tokens that are classified as correct topic\n",
        "target_p = 0.8\n",
        "#hyperparameter that determines class prior, set to uniform by default\n",
        "class_bias = 0\n",
        "\n",
        "if gen_length>1024:\n",
        "  length = 1024\n",
        "else:\n",
        "  length = gen_length\n",
        "\n",
        "\n",
        "print('Please input a movie genre to select from (try sci-fi, horror, comedy and romance for example).')\n",
        "secondary_code = 'sci-fi'\n",
        "secondary_code = input('Movie genre: ')\n",
        "bpe_tokens = tokenizer.encode(secondary_code)\n",
        "\n",
        "\n",
        "\n",
        "multi_code = tokenizer.encode(secondary_code)\n",
        "attr_class = 1\n",
        "\n",
        "text = \"\"\n",
        "num_turns = 5 #generation lengths are unexpected so no garuntees youll get to the end of your conversation\n",
        "for i in range(num_turns):\n",
        "  prompt = input('Your turn: ')\n",
        "  prompt = text + \" \" + prompt + \" \"  + tokenizer.eos_token\n",
        "  text_ids = tokenizer.encode(prompt)\n",
        "  encoded_prompts=torch.LongTensor(text_ids).unsqueeze(0).to(device)\n",
        "\n",
        "  generated_sequence = model.generate(input_ids=encoded_prompts,\n",
        "                                          pad_lens=None,\n",
        "                                            max_length= length,\n",
        "                                            top_k=None,\n",
        "                                            top_p=None,\n",
        "                                            repetition_penalty= 1.2,\n",
        "                                            rep_penalty_scale= 10,\n",
        "                                            eos_token_ids = tokenizer.eos_token_id,\n",
        "                                            pad_token_id = 0,\n",
        "                                            do_sample= False,\n",
        "                                            penalize_cond= True,\n",
        "                                            gedi_model= gedi_model,\n",
        "                                            tokenizer= tokenizer,\n",
        "                                            disc_weight= disc_weight,\n",
        "                                            filter_p = filter_p,\n",
        "                                            target_p = target_p,\n",
        "                                            class_bias = class_bias,\n",
        "                                            attr_class = attr_class,\n",
        "                                            code_0 = code_undesired,\n",
        "                                            code_1 = code_desired,\n",
        "                                            multi_code=multi_code\n",
        "                                            )\n",
        "\n",
        "  text =  tokenizer.decode(generated_sequence.tolist()[0], clean_up_tokenization_spaces=True)\n",
        "  print('\\n')\n",
        "  print(text)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/scene-bot/GeDi\n",
            "no logit scale initialized for gpt2\n",
            "Please input a movie genre to select from (try sci-fi, horror, comedy and romance for example).\n",
            "Movie genre: sci-fi\n",
            "Your turn: hello folks\n",
            "GeDi estimates the probability that it sample is desired class is: 0.475144624710083\n",
            "\n",
            "\n",
            " hello folks<|endoftext|>Hiya Jimmy. <|endoftext|>\n",
            "Your turn: my names bert\n",
            "GeDi estimates the probability that it sample is desired class is: 0.48719778656959534\n",
            "\n",
            "\n",
            " hello folks<|endoftext|>Hiya Jimmy.<|endoftext|> my names bert<|endoftext|> Hiya Teddy. <|endoftext|>\n",
            "Your turn: ok asshole\n",
            "GeDi estimates the probability that it sample is desired class is: 0.4918922185897827\n",
            "\n",
            "\n",
            " hello folks<|endoftext|>Hiya Jimmy.<|endoftext|> my names bert<|endoftext|> Hiya Teddy.<|endoftext|> ok asshole<|endoftext|> Teddy meet my brother Gabe. <|endoftext|>\n",
            "Your turn: i dont want to meet gabe\n",
            "GeDi estimates the probability that it sample is desired class is: 0.49781450629234314\n",
            "\n",
            "\n",
            " hello folks<|endoftext|>Hiya Jimmy.<|endoftext|> my names bert<|endoftext|> Hiya Teddy.<|endoftext|> ok asshole<|endoftext|> Teddy meet my brother Gabe.<|endoftext|> i dont want to meet gabe<|endoftext|> Well then let's not invite him. You and your brother should get outta here before Silent Night happens. Now shhiiittt be careful there they might see you running around in your filthy bed like some stray animal. <|endoftext|>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a6d9a5a64e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mnum_turns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m#generation lengths are unexpected so no garuntees youll get to the end of your conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_turns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your turn: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mtext_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwT4xXOq01QR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}